{"metadata":{"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/zostaw/learning-embeddings?scriptVersionId=167617649\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install torch pandas matplotlib optuna graphviz\ntry:\n    dbutils.library.restartPython()\nexcept NameError:\n    print(\"dbutils not found\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import parallel_coordinates\nfrom pandas.plotting import radviz\nimport matplotlib.image as mpimg\nimport optuna\nimport urllib.request;\nimport pickle\nfrom pathlib import Path\nimport graphviz","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding space - sparse vs dense","metadata":{}},{"cell_type":"code","source":"text = \"\"\"But these new vectors are a big pain: you could think of thousands of different semantic attributes that might be relevant to determining similarity, and how on earth would you set the values of the different attributes? Central to the idea of deep learning is that the neural network learns representations of the features, rather than requiring the programmer to design them herself. So why not just let the word embeddings be parameters in our model, and then be updated during training? This is exactly what we will do. We will have some latent semantic attributes that the network can, in principle, learn. Note that the word embeddings will probably not be interpretable. That is, although with our hand-crafted vectors above we can see that mathematicians and physicists are similar in that they both like coffee, if we allow a neural network to learn the embeddings and see that both mathematicians and physicists have a large value in the second dimension, it is not clear what that means. They are similar in some latent semantic dimension, but this probably has no interpretation to us.\"\"\"\n\n# Build vocabulary out of unique words\nvocab = list(set(text.split()))\nvocab_size = len(vocab)\nprint(vocab)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### One-hot embedding","metadata":{}},{"cell_type":"code","source":"vocab_dict = {word: id for id, word in enumerate(vocab)}\nprint(vocab_dict)\n\none_hot_embed = torch.eye(len(vocab_dict), len(vocab_dict))\nprint(one_hot_embed, one_hot_embed.size())\n\ndef one_hot(x):\n    return one_hot_embed[vocab_dict[x]]\n\nx = 'mathematicians'\ny = one_hot(x)\nprint(y)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One-hot is a lookup table that has a distinct dimension for each word in a vocabulary - it's sparse in a sense that every word is isolated from others.\n","metadata":{}},{"cell_type":"markdown","source":"#### Dense embedding space\n\nSparse embedding space is a poor way to displaying relationship between elements - they are izolated from each other. If you want to represent meaningfull connections between the words, you want to create a shared space for all of them.","metadata":{}},{"cell_type":"code","source":"# Dummy embedding model\nclass dense_embedding_model(nn.Module):\n    def __init__(self, vocab_size, dims):\n        super().__init__()\n        self.embed_table = torch.randn(vocab_size, dims)\n\n    def forward(self, word):\n        return self.embed_table[word]\n\n# Helping functions\ndef word_to_int(word, vocab):\n    return vocab[word]\n\ndef word_to_tensor(word, vocab):\n    return torch.tensor(vocab[word]).reshape(1)\n\ndef words_to_tensors(words, vocab):\n    ints = []\n    for word in words:\n        ints.append(word_to_int(word, vocab))\n    return torch.tensor(ints)\n\n\n# Initiate model\ndims = 5\ntorch.manual_seed(69)\nembed_model = dense_embedding_model(vocab_size, dims)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print embeddings for example words\nexample_words = ['mathematicians', 'and', 'physicists', 'can', 'learn']\nexample_words_ints = words_to_tensors(example_words, vocab_dict)\n\ny = embed_model(example_words_ints)\n\nfor word, emb in zip(example_words, y):\n    print(f'{word}: {emb}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print full embedding space\nwords_ints = words_to_tensors(vocab, vocab_dict)\ncomplete_space = embed_model(words_ints)\n\ndf = pd.DataFrame([{\"name\": name, \"id\": i} for name, i in vocab_dict.items()])\n\nfor embed_dim in range(complete_space.size(dim=1)):\n    df[f\"embed_dim_{embed_dim}\"] = complete_space[:, embed_dim]\n\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 9))  # Adjust the width and height as needed\nparallel_coordinates(df.drop(\"id\", axis=1), \"name\")\nplt.legend(fontsize='xx-small', ncol=10)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, one can thing about the embeddings as words being represented in multiple dimensions.  \nIn above example there's no correlation between them - obviously - we initiated embeddings for each word randomly.  \n","metadata":{}},{"cell_type":"markdown","source":"## Dimensions as similarities\n\nIdeally we want the embeddings represent something meaningfull - in similar way as we can see certain features of things when analysing real life data and recognize similarities and differences.   \nFor example here we have 3 species of Irises.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(10, 7))\nimg0 = mpimg.imread('resources/iris-setosa.jpeg')\nimg1 = mpimg.imread('resources/iris-versicolor.jpeg')\nimg2 = mpimg.imread('resources/iris-virginica.jpeg')\nfig.add_subplot(1, 3, 1)\nplt.imshow(img0)\nplt.title(\"Iris Setosa\")\nfig.add_subplot(1, 3, 2)\nplt.imshow(img1)\nplt.title(\"Iris Versicolor\")\nfig.add_subplot(1, 3, 3)\nplt.imshow(img2)\nplt.title(\"Iris Virginica\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we plot parameters of each individual of iris from dataset as separate dimensions, we notice the differences between the *categories* and similarities between *individuals*.","metadata":{}},{"cell_type":"code","source":"# Similarities between individuals of Irises in 4 dimensions\nfig = plt.figure(figsize=(6, 3))\niris = pd.read_csv(\"resources/Iris.csv\")\nparallel_coordinates(iris.drop(\"Id\", axis=1), \"Species\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Different species of Iris are similar to each other, but individuals are more similar among themselves and this can be seen in 4 dimensions: SepalLength, Sepal Width, Petal Length and Petal Width.  \nIt would be wonderful if we could achieve something similar in language - to represent some relationship between them.  \nNot necessarily similarity between words per say, but to associate semantic meaning between them.  \n\n# Word2Vec","metadata":{}},{"cell_type":"markdown","source":"This is what we can achieve by training embeddings in special way.  \nWord2Vec is one of methods to achieve this.  \nAdditionally to defined embeddings layer as we did in **Dense embedding space** example, we add additional linear layer with outputs of same size as inputs.  \nThis architecture will allow us to define a feedback loop between input words and output (predicted) words.  ","metadata":{}},{"cell_type":"markdown","source":"## Model definition","metadata":{}},{"cell_type":"code","source":"class CBOW(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, context_size, batch_size, seed):\n        super().__init__()\n        torch.manual_seed(seed)\n        # layers\n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.linear1 = nn.Linear(context_size*embedding_dim, vocab_size)\n        # save parameters for later\n        self.context_size = context_size\n        self.embedding_dim = embedding_dim\n        self.batch_size = batch_size\n\n        print(f\"[model] embed_layer size: {vocab_size}-> {embedding_dim}\")\n        print(f\"[model] linear1 size: {embedding_dim}-> {vocab_size}\")\n\n    def forward(self, inputs):\n        embed = self.embedding_layer(inputs)\n        embed_flat = embed.view(-1, self.embedding_dim*self.context_size)\n        logits = self.linear1(embed_flat)\n        #out = F.softmax(logits, dim=1)\n        return logits\n\n    def get_embed(self, inputs):\n        embed = self.embedding_layer(inputs)\n        return embed\n\nclass CBOW2(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, context_size, batch_size, seed):\n        super().__init__()\n        torch.manual_seed(seed)\n        # layers\n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n        # save parameters for later\n        self.context_size = context_size\n        self.embedding_dim = embedding_dim\n        self.batch_size = batch_size\n\n        print(f\"[model] embed_layer size: {vocab_size}-> {embedding_dim}\")\n        print(f\"[model] linear1 size: {embedding_dim}-> {vocab_size}\")\n\n    def forward(self, inputs):\n        embed = self.embedding_layer(inputs)\n        logits = self.linear1(embed)\n        out = F.softmax(logits, dim=1)\n        return out\n\n    def get_embed(self, inputs):\n        embed = self.embedding_layer(inputs)\n        return embed\n\ndef train(dataloader, model, loss_fn, optimizer, epochs, batch_size, device):\n    model.train()\n\n    losses = []\n    for epoch in range(epochs):\n        print(f\"[train] Epoch: {epoch}/{epochs}\")\n        total_loss = 0\n\n        for inputs, output in dataloader:\n            inputs = inputs.to(device)\n            output = output.to(device)\n            print(f\"[train] inputs: {inputs.size()}\")\n            print(f\"[train] output: {output.size()}\")\n            if (inputs_size:=inputs.size(dim=0)) != batch_size:\n                print(f\"[train] inputs size ({inputs_size}) not equal to batch size ({batch_size}) - last batch?? skipping\")\n                continue\n            optimizer.zero_grad()\n            pred = model(inputs)\n            print(f\"[train] pred: {pred.size()}\")\n\n            loss = loss_fn(pred, output)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        losses.append(total_loss)\n\n    return losses\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In practice the model is trained to minimize $Loss$ based on prediction of the middle word from surrounding ones. See below gif to get the idea behind it.","metadata":{}},{"cell_type":"code","source":"# It takes ~35 seconds to generate the gif on Intel i7 with 16 GB Ram\n# Otherwise the gif is already generated and can be found in the resources folder\n# Also open gif separately, notebook doesn't load gifs\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom visualize_embedding_layers import draw_pipeline\nBGCOLOR=\"black\"\nfig, ax = plt.subplots(figsize=(16, 9))\nfig.set_facecolor(BGCOLOR)\n\ndef update(frame):\n    plt.cla()\n    fig.set_facecolor(BGCOLOR)\n    ax.axis('off')\n    draw_pipeline(text=text, \n                    word_start_id=frame, \n                    context_window_size=5, \n                    embed_dims=3, \n                    ax=ax, \n                    bgcolor=BGCOLOR)\n\nani = animation.FuncAnimation(fig=fig, func=update, frames=40, interval=100)\nwriter = animation.PillowWriter(fps=1)\nani.save('resources/model_training_scheme.gif',writer=writer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset definition","metadata":{}},{"cell_type":"code","source":"class CBOWTrainingDataset(Dataset):\n    def word_to_tensor(self,word, vocab):\n        return torch.tensor(vocab[word]).reshape(1)\n\n    def word_to_one_hot(self, word, vocab):\n        return self.one_hot_table[vocab[word]]\n\n    def build_ds(self, words):\n        targets, contexts = None, None\n        for i in range(self.half_context_size, self.text_size-self.half_context_size):\n            context_left = torch.vstack([self.word_to_tensor(words[i - j - 1],\n                                                        self.vocab_dict) for j in range(self.half_context_size)])\n            context_right = torch.vstack([self.word_to_tensor(words[i + j + 1],\n                                                        self.vocab_dict) for j in range(self.half_context_size)])\n            context = torch.vstack((context_left, context_right)).to(torch.int64)\n\n            target = self.word_to_one_hot(words[i], self.vocab_dict).to(self.dtype)\n\n            if contexts is None:\n                contexts = context.reshape(1, self.context_size)#, self.vocab_size)\n                targets = target.reshape(1, self.vocab_size)\n            else:\n                contexts = torch.vstack((contexts, context.reshape(1, self.context_size)))#, self.vocab_size)))\n                targets = torch.vstack((targets, target.reshape(1, self.vocab_size)))\n\n        print(f\"[DS] Last text word id: {i}\")\n        print(f\"[DS] contexts: {contexts.size()}\")\n        print(f\"[DS] targets: {targets.size()}\")\n        print(f\"[DS] vocab_size: {self.vocab_size}\")\n        return contexts, targets\n\n\n    def __init__(self, words, context_size, dtype=torch.float32):\n        self.text_size = len(words)\n        print(f\"[DS] text size: {self.text_size}\")\n        self.dtype = dtype\n        self.context_size = int(context_size)\n        self.half_context_size = int(context_size/2)\n        vocab = list(set(words))\n        self.vocab_size = len(vocab)\n        self.vocab_dict = {name: i for i, name in enumerate(vocab)}\n        # one-host table to generate vocab-size targets\n        self.one_hot_table = torch.eye(self.vocab_size, self.vocab_size)\n\n        #build_ds = jax.jit(self.build_ds_jax)\n        build_ds = self.build_ds\n        self.contexts, self.targets = build_ds(words)\n\n\n    def __len__(self):\n        return len(self.contexts)\n\n    def __getitem__(self, idx):\n        # inputs are (text_size-context_size)/context_size/int\n        # targets are (text_size-context_size)/1/vocab_size\n        context = self.contexts[idx]\n        target = self.targets[idx]\n        return context, target","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset text source","metadata":{}},{"cell_type":"code","source":"text_sources = [\"shapespeare\", \"The Three Little Pigs\"]\n# pick one of above\ntext_source = \"The Three Little Pigs\"\n\nif text_source == \"shakespeare\":\n    text_url = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n    text_file = \"shakespeare.txt\"\n    text_words_file = \"text_words.pkl\"\n    text_size = 10_004\n\n    if Path(text_words_file).exists():\n        with open(text_words_file, \"rb\") as file:\n            text_words = pickle.load(file)\n    else:\n        urllib.request.urlretrieve(text_url, text_file)\n        with open(file=text_file, mode='r') as file:\n            text = file.read()\n            # split\n            text_words = list(text.split())\n            # save for quick retrieval\n            with open(text_words_file, 'wb') as file:\n                pickle.dump(text_words, file)\n\n    text_words = text_words[:text_size]\n\n    probe_word = \"lartius\"\n\n\nelif text_source == \"The Three Little Pigs\":\n    text = \"\"\"\n                        The Three Little Pigs\n                        ---------------------\n                        Once upon a time\n                        there was a family of three little pigs\n                            *oink sound effect*\n                        one little pig lived in a house of straw\n                        another little pig lived in a house of wood\n                        the third little pig lived in a house of brick\n                        But then there came along a big bad wolf.\n                            *howling wolf*\n                        The wolf went knocking on the first pig's door\n                        \"Little pig, little pig,\" he said.  \"let me come in!\"\n                        \"Not by the hairs on my chinny chin chin.\" the first pig said.\n                        \"Then I'll huff, and I'll puff, and I'll blow your house in,\" said the wolf.\n                        So he huffed, and he puffed,\n                            *wind sound effect*\n                        and he blew the house down and ate the little pig right up.\n                        The next day, the big bad wolf came knocking on the second pig's door.\n                            *knock sound effect*\n                        \"Little pig, little pig,\" he said, \"let me come in!\"\n                        \"Not by the hairs on my chinny chin chin,\" the second pig said.\n                        \"Then I'll huff and I'll puff, and I'll blow your house in,\" said the wolf.\n                        So he huffed and he puffed,\n                            *wind sound effect*\n                        and he blew the house down, and ate the little pig right up.\n                        The next day, the big bad wolf came knocking on the third pig's door.\n                        \"Little pig, little pig,\" he said, \"let me come in!\"\n                        \"Not by the hairs on my chinny chin chin,\" the third pig said.\n                        \"Then I'll huff and I'll puff, and I'll blow your house in,\" said the wolf.\n                        So he huffed and he puffed,\n                            *wind sound effect*\n                        but the house of bricks would not fall.\n                        The wolf went home and thought about how to trick the third little pig.\n                        The next day, the big bad wolf came knocking on the door again.\n                        \"Ho ho ho,\" he said. \"It's me, Santa Claus! I have presents!\"\n                        \"If you are really Santa, then come down my chimney chim chim,\" the third pig said.\n                        So he jumped up on the roof and climbed in the chimney.\n                        But the third little pig knew it was the wolf, and put a pot of hot water under the chimney.\n                        Down the wolf came, straight into the water, and got cooked right up.\n                            *screaming, burning alive sound effect*\n                        As the pig began to feast on his wolf dinner, his brothers jumped out and they were reunited.\n                        And the three little pigs lived happily ever after.\n                            *music*\n\n                    \"\"\"\n    text_words = list(text.split())\n    probe_word = \"wolf\"\n\nelse:\n    raise ValueError(f\"text {text_source} not found, expected one of {text_sources}.\")\n\ntext_words = [word.lower() for word in text_words if word.isalpha()]\nprint(f\"test len: {len(text_words)}, first 10 words: {text_words[:10]}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grid search for optimal params.","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    # Set hyperparameters\n    embed_dim = trial.suggest_int('embed_dim', 5, 55, step=10)\n    learning_rate = trial.suggest_float('lr', 1e-3, 1, log=True)\n    #epochs = trial.suggest_int('epochs', 1, 2)\n    epochs = 2\n    half_context_size = trial.suggest_int('half_context_size', 2, 5)\n    context_size = 2*half_context_size\n    #batch_size = trial.suggest_int('batch_size', 1000, max(2000, min(4000, len(text_words)-context_size)))\n    batch_size = trial.suggest_int('batch_size', 10, 1000)\n    seed = trial.suggest_int('seed', 70, 75)\n\n    device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n    print(device)\n\n    # Build training list for Word2Vec Continuous Bag-of-Words method\n    print(f\"[objective] batch_size: {batch_size}\")\n    print(f\"[objective] embed_dim: {embed_dim}\")\n    print(f\"[objective] context size: {context_size}\")\n\n    if context_size in datasets:\n        dataset = datasets[context_size]\n    else:\n        dataset = CBOWTrainingDataset(text_words, context_size)\n        datasets[context_size] = dataset\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    # Verify sizes\n    for context, target in dataloader:\n        print(f\"[objective] context: {context.size()}, \\ntarget: {target.size()}, \\nvocab_size: {dataset.vocab_size}\")\n        break\n\n\n    # Define model, loss function and optimizer\n    model = CBOW(dataset.vocab_size, embedding_dim=embed_dim, context_size=context_size, batch_size=batch_size, seed=seed)\n    model.to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Training\n    losses = train(dataloader=dataloader,\n                   model=model,\n                   loss_fn=loss_fn,\n                   optimizer=optimizer,\n                   epochs=epochs,\n                   batch_size=batch_size,\n                   device=device)\n\n    return losses[-1]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run grid search and/or training","metadata":{}},{"cell_type":"code","source":"# Grid search is very long, it looks for optimal hyperparameters\ngrid_seach_mode = False\n\n# datasets dict with context_size as key and CBOWTrainingDataset instance for that context size as value - they are reused in the optuna trials or during final training\ndatasets = {}\ndevice = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\nprint(device)\n\nif grid_seach_mode:\n    # Optuna study for hyperparameter optimization\n    study = optuna.create_study(storage=\"sqlite:///db.sqlite3\",\n                                study_name=\"Embeddings-similarity2\",\n                                direction=\"minimize\",\n                                load_if_exists = True)\n    study.optimize(objective, n_trials=100)\n\n    print(f\"Best value: {study.best_value} (params: {study.best_params})\")\n    embed_dim = study.best_params['embed_dim']\n    lr = study.best_params['lr']\n    context_size = 2*study.best_params['half_context_size']\n    batch_size = study.best_params['batch_size']\n    seed = study.best_params['seed']\n\nelse:\n    # Once you found optimal parameters disable grid_search_mode and set below to optimal\n    embed_dim=3\n    lr=0.026727287161114725\n    context_size=6\n    batch_size=len(text_words)-context_size\n    seed=73\n\nepochs = 100\n\n\n# Build training list for Word2Vec Continuous Bag-of-Words method\nprint(f\"[objective] batch_size: {batch_size}\")\nprint(f\"[objective] embed_dim: {embed_dim}\")\nprint(f\"[objective] context size: {context_size}\")\n\nif context_size in datasets:\n    dataset = datasets[context_size]\nelse:\n    dataset = CBOWTrainingDataset(text_words, context_size)\n    datasets[context_size] = dataset\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n# Verify sizes\nfor context, target in dataloader:\n    print(f\"[objective] context: {context.size()}, \\ntarget: {target.size()}, \\nvocab_size: {dataset.vocab_size}\")\n    break\n\n\n# Define model, loss function and optimizer\nmodel = CBOW(dataset.vocab_size, embedding_dim=embed_dim, context_size=context_size, batch_size=batch_size, seed=seed)\nmodel.to(device)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# Training\nlosses = train(dataloader=dataloader,\n                model=model,\n                loss_fn=loss_fn,\n                optimizer=optimizer,\n                epochs=epochs,\n                batch_size=batch_size,\n                device=device)\n\n# Display performance\nplt.plot(losses)\nprint(f\"Loss after last epoch: {losses[-1]}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Prepare vocab dictionary for embeddings visualization","metadata":{}},{"cell_type":"code","source":"model.eval()\n\npresentation_vocab_dict = dataset.vocab_dict\npresentation_reverse_vocab_dict = {i: name for name, i in presentation_vocab_dict.items()}\n\ndict_words = []\npresentation_vocab = list(set(text_words[:10000]))\nfor id, word in enumerate(presentation_vocab):\n    word_id = torch.tensor(presentation_vocab_dict[word]).to(device)\n    embed = model.get_embed(word_id)\n    # append\n    dict_row = {\"id\": id, \"word\": word, \"raw_embed\": embed}\n    dict_row.update({f\"dim_{dim_id}\": dim_val.item() for dim_id, dim_val in enumerate(embed)})\n    dict_words.append(dict_row)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Define word-to-embedding functions","metadata":{}},{"cell_type":"code","source":"df_words = pd.DataFrame.from_dict(dict_words)\n\ndef word_to_embed(word):\n    return dict_words[presentation_vocab_dict[word]][\"raw_embed\"]\n\ndef embed_to_word(embed):\n    return [record for record in dict_words if torch.equal(record.get(\"raw_embed\"), embed)][0][\"word\"]\n\n# Test functions - transpose first embedding to word and back\nassert(text_words[0] == embed_to_word(word_to_embed(text_words[0])))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting similarities for particular word","metadata":{}},{"cell_type":"code","source":"# Find similarities between given word and others\n#probe_word = THIS IS DEFINED ALONG WITH DATASET\nn = 5\n\nword_embed = word_to_embed(probe_word)\n\n# Build list of similarities between probe_word and rest of words from vocabulary\nsimilarities = []\nfor word in dict_words:\n    similarities.append((word[\"word\"], F.cosine_similarity(word_embed, word[\"raw_embed\"], dim=0)))\n\nprint(f\"First {n} similarities for \\\"{probe_word}\\\":\\n{similarities[:n]}\\n\")\n\n# Sort the list of tuples in descending order based on the values\ninverse = [(value, key) for key, value in similarities]\ninverse.sort(reverse=True)\n\n# Get the keys corresponding to the n greatest values\ntop_n_keys = [key for _, key in inverse[:n]]\n\nprint(f\"Keys corresponding to the {n} greatest values:\\n{top_n_keys}\\nand least: \\\"{inverse[-1][1]}\\\"\\n\")\n\n# Transpose to DataFrame for plotting\ndf_words = pd.DataFrame(dict_words)\nprint(\"DataFrame:\\n\", df_words, \"\\n\")\n\n# Plot similarities for probe_word\nplt.figure(figsize=(15, 9))  # Adjust the width and height as needed\ndf_print = df_words.filter(items=['id', 'word', 'dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4']) # plot only first 5 dimensions\nparallel_coordinates(df_print[df_print[\"word\"].isin(top_n_keys)].drop([\"id\"], axis=1), 'word')\n#plt.legend(ncol=10)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dummy plotting by displaying first 2 dimensions\n\nPlot the first 2 dimensions of the embeddings.","metadata":{}},{"cell_type":"code","source":"# Pick some words to plot their embeddings\n#df_low_dim = df_print[df_print[\"word\"].isin([\"Caroline\", \"went\", \"school\", \"Brand\", \"Stacy\", \"to\", \"He\"])]\ndf_low_dim = df_print\n# data\ndata = df_low_dim.drop([\"id\", \"word\"], axis=1)\n# labels\nlabels = df_low_dim[\"word\"]\n\n# Plot\nfig = plt.figure(figsize=(50, 50))\nax = fig.add_subplot(1, 1, 1)#, projection='3d')\n\nax.scatter(data['dim_0'], data['dim_1'])\n#parallel_coordinates(df_print[df_print[\"word\"].isin(top_n_keys)].drop([\"id\"], axis=1), 'word')\n\n# annotator function that draws a label and an arrow\n# that points from the label to its corresponding point\ndef annotate(ax, label, x, y, xytext):\n    ax.annotate(label, xy=(x,y),\n                xytext=xytext, textcoords='offset points',\n                fontsize=10,\n                )\n\n# conditionally position labels\nfor label, x, y in zip(labels, data[\"dim_0\"], data[\"dim_1\"]):\n    if y > 0.9:\n        annotate(ax, label, x, y, (-5, -15))\n    else:\n        annotate(ax, label, x, y, (-5, 10))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting through reduction of dimmensionality using TSNE","metadata":{}},{"cell_type":"code","source":"\n# Importing Necessary Modules.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\n\n# Data-preprocessing: Standardizing the data\nfrom sklearn.preprocessing import StandardScaler\n\nstandardized_data = StandardScaler().fit_transform(data)[:10]\nprint(standardized_data.shape)\n\n\n# Picking the top 1000 points as TSNE\n# takes a lot of time for 15K points\n#data_1000 = standardized_data[0:1000, :]\n#labels_1000 = labels[0:1000]\n\nmodel = TSNE(n_components = 2, random_state = 0, perplexity=min(standardized_data.shape[1], 10))\n# configuring the parameters\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations\n# for the optimization = 1000\n\n#tsne_data = model.fit_transform(data)\ntsne_data = model.fit_transform(standardized_data)\n\n# creating a new data frame which\n# help us in plotting the result data\ntsne_data = np.vstack((tsne_data.T, labels[:10])).T\ntsne_df = pd.DataFrame(data = tsne_data,\n     columns =(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Plotting the result of tsne\nsns.scatterplot(data=tsne_df, x='Dim_1', y='Dim_2',\n               hue='label', palette=\"bright\")\nplt.legend(fontsize='xx-small', ncol=10, bbox_to_anchor=(1.1, 1.05))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}