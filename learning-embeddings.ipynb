{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch pandas matplotlib optuna graphviz\n",
    "try:\n",
    "    dbutils.library.restartPython()\n",
    "except NameError:\n",
    "    print(\"dbutils not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from pandas.plotting import radviz\n",
    "import matplotlib.image as mpimg\n",
    "import optuna\n",
    "import urllib.request;\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "# modules for t-SNE\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "RESOURCES_DIR = \"./resources/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding space - sparse vs dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"But these new vectors are a big pain: you could think of thousands of different semantic attributes that might be relevant to determining similarity, and how on earth would you set the values of the different attributes? Central to the idea of deep learning is that the neural network learns representations of the features, rather than requiring the programmer to design them herself. So why not just let the word embeddings be parameters in our model, and then be updated during training? This is exactly what we will do. We will have some latent semantic attributes that the network can, in principle, learn. Note that the word embeddings will probably not be interpretable. That is, although with our hand-crafted vectors above we can see that mathematicians and physicists are similar in that they both like coffee, if we allow a neural network to learn the embeddings and see that both mathematicians and physicists have a large value in the second dimension, it is not clear what that means. They are similar in some latent semantic dimension, but this probably has no interpretation to us.\"\"\"\n",
    "\n",
    "# Build vocabulary out of unique words\n",
    "vocab = list(set(text.split()))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {word: id for id, word in enumerate(vocab)}\n",
    "print(vocab_dict)\n",
    "\n",
    "one_hot_embed = torch.eye(len(vocab_dict), len(vocab_dict))\n",
    "print(one_hot_embed, one_hot_embed.size())\n",
    "\n",
    "def one_hot(x):\n",
    "    return one_hot_embed[vocab_dict[x]]\n",
    "\n",
    "x = 'mathematicians'\n",
    "y = one_hot(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot is a lookup table that has a distinct dimension for each word in a vocabulary - it's sparse in a sense that every word is isolated from others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense embedding space\n",
    "\n",
    "Sparse embedding space is a poor way to displaying relationship between elements - they are izolated from each other. If you want to represent meaningfull connections between the words, you want to create a shared space for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy embedding model\n",
    "class dense_embedding_model(nn.Module):\n",
    "    def __init__(self, vocab_size, dims):\n",
    "        super().__init__()\n",
    "        self.embed_table = torch.randn(vocab_size, dims)\n",
    "\n",
    "    def forward(self, word):\n",
    "        return self.embed_table[word]\n",
    "\n",
    "# Helping functions\n",
    "def word_to_int(word, vocab):\n",
    "    return vocab[word]\n",
    "\n",
    "def word_to_tensor(word, vocab):\n",
    "    return torch.tensor(vocab[word]).reshape(1)\n",
    "\n",
    "def words_to_tensors(words, vocab):\n",
    "    ints = []\n",
    "    for word in words:\n",
    "        ints.append(word_to_int(word, vocab))\n",
    "    return torch.tensor(ints)\n",
    "\n",
    "\n",
    "# Initiate model\n",
    "dims = 5\n",
    "torch.manual_seed(69)\n",
    "embed_model = dense_embedding_model(vocab_size, dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print embeddings for example words\n",
    "example_words = ['mathematicians', 'and', 'physicists', 'can', 'learn']\n",
    "example_words_ints = words_to_tensors(example_words, vocab_dict)\n",
    "\n",
    "y = embed_model(example_words_ints)\n",
    "\n",
    "for word, emb in zip(example_words, y):\n",
    "    print(f'{word}: {emb}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print full embedding space\n",
    "words_ints = words_to_tensors(vocab, vocab_dict)\n",
    "complete_space = embed_model(words_ints)\n",
    "\n",
    "df = pd.DataFrame([{\"name\": name, \"id\": i} for name, i in vocab_dict.items()])\n",
    "\n",
    "for embed_dim in range(complete_space.size(dim=1)):\n",
    "    df[f\"embed_dim_{embed_dim}\"] = complete_space[:, embed_dim]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))  # Adjust the width and height as needed\n",
    "parallel_coordinates(df.drop(\"id\", axis=1), \"name\")\n",
    "plt.legend(fontsize='xx-small', ncol=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, one can thing about the embeddings as words being represented in multiple dimensions.  \n",
    "In above example there's no correlation between them - obviously - we initiated embeddings for each word randomly.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions as similarities\n",
    "\n",
    "Ideally we want the embeddings represent something meaningfull - in similar way as we can see certain features of things when analysing real life data and recognize similarities and differences.   \n",
    "For example here we have 3 species of Irises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "img0 = mpimg.imread(RESOURCES_DIR + 'iris-setosa.jpeg')\n",
    "img1 = mpimg.imread(RESOURCES_DIR + 'iris-versicolor.jpeg')\n",
    "img2 = mpimg.imread(RESOURCES_DIR + 'iris-virginica.jpeg')\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(img0)\n",
    "plt.title(\"Iris Setosa\")\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Iris Versicolor\")\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(img2)\n",
    "plt.title(\"Iris Virginica\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot parameters of each individual of iris from dataset as separate dimensions, we notice the differences between the *categories* and similarities between *individuals*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarities between individuals of Irises in 4 dimensions\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "iris = pd.read_csv(RESOURCES_DIR + 'Iris.csv')\n",
    "parallel_coordinates(iris.drop(\"Id\", axis=1), \"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different species of Iris are similar to each other, but individuals are more similar among themselves and this can be seen in 4 dimensions: SepalLength, Sepal Width, Petal Length and Petal Width.  \n",
    "It would be wonderful if we could achieve something similar in language - to represent some relationship between them.  \n",
    "Not necessarily similarity between words per say, but to associate semantic meaning between them.  \n",
    "\n",
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we can achieve by training embeddings in special way.  \n",
    "Word2Vec is one of methods to achieve this.  \n",
    "Additionally to defined embeddings layer as we did in **Dense embedding space** example, we add additional linear layer with outputs of same size as inputs.  \n",
    "This architecture will allow us to define a feedback loop between input words and output (predicted) words.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, batch_size, seed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        # layers\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 4*embedding_dim)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        self.linear2 = nn.Linear(4*embedding_dim, vocab_size)\n",
    "        # save parameters for later\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        print(f\"[model] embed_layer size: {vocab_size}-> {embedding_dim}\")\n",
    "        print(f\"[model] linear1 size: {embedding_dim}-> {vocab_size}\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embed = self.embedding_layer(inputs)\n",
    "        embed_sum = torch.mean(embed, dim=1)\n",
    "        logits = self.linear1(embed_sum)\n",
    "        logits = F.relu(logits)\n",
    "        #logits = self.dropout(logits)\n",
    "        logits = self.linear2(logits)\n",
    "        # using crossentropy, so softmax is unnecessary\n",
    "        #out = F.softmax(logits, dim=1)\n",
    "        out = logits\n",
    "        return out\n",
    "\n",
    "    def get_embed(self, inputs):\n",
    "        embed = self.embedding_layer(inputs)\n",
    "        return embed\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, epochs, batch_size, device, print_every=10, debug=False):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        if debug: print(f\"[train] Epoch: {epoch}/{epochs}\")\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, output in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            output = output.reshape(-1,).to(device)\n",
    "            if debug: print(f\"[train] inputs: {inputs.size()}, type: {inputs.type()}\")\n",
    "            if debug: print(f\"[train] output: {output.size()}, type: {output.type()}\")\n",
    "            if (inputs_size:=inputs.size(dim=0)) != batch_size:\n",
    "                print(f\"[train] inputs size ({inputs_size}) not equal to batch size ({batch_size}) - last batch?? skipping\")\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(inputs)\n",
    "            if debug: print(f\"[train] pred: {pred.size()}, type: {pred.type()}\")\n",
    "\n",
    "            loss = loss_fn(pred, output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if (epoch+1)%print_every == 0:\n",
    "            print(f\"[train] Epoch {epoch} - loss: {total_loss/batch_size}\")\n",
    "\n",
    "        losses.append(total_loss/batch_size)\n",
    "\n",
    "    return model, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice the model is trained to minimize $Loss$ based on prediction of the middle word from surrounding ones. See below gif to get the idea behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes ~35 seconds to generate the gif on Intel i7 with 16 GB Ram\n",
    "# Otherwise the gif is already generated and can be found in the resources folder\n",
    "# Also open gif separately, notebook doesn't load gifs\n",
    "def generate_animation():\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "    from visualize_embedding_layers import draw_pipeline\n",
    "    BGCOLOR=\"black\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "    fig.set_facecolor(BGCOLOR)\n",
    "\n",
    "    def update(frame):\n",
    "        plt.cla()\n",
    "        fig.set_facecolor(BGCOLOR)\n",
    "        ax.axis('off')\n",
    "        draw_pipeline(text=text,\n",
    "                        word_start_id=frame,\n",
    "                        context_window_size=5,\n",
    "                        embed_dims=3,\n",
    "                        ax=ax,\n",
    "                        bgcolor=BGCOLOR)\n",
    "\n",
    "    ani = animation.FuncAnimation(fig=fig, func=update, frames=40, interval=100)\n",
    "    writer = animation.PillowWriter(fps=1)\n",
    "    ani.save(RESOURCES_DIR + 'model_training_scheme.gif', writer=writer)\n",
    "\n",
    "# Uncomment below if you want to see directly or play with parameters - it will render animation here in notebook,\n",
    "# otherwise in markdown cell below is same gif preloaded from file when notebook is started\n",
    "# I commented generator out, because it doesn't render well in Github/Jupyter (also it takes long time to generate each time)\n",
    "#generate_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is training loop demonstration (see comment above if you want to change it).\n",
    "\n",
    "![training gif](./resources/model_training_scheme.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWTrainingDataset(Dataset):\n",
    "    def word_to_tensor(self, word, vocab):\n",
    "        return torch.tensor(vocab[word]).reshape(1)\n",
    "\n",
    "    def word_to_one_hot(self, word, vocab):\n",
    "        return self.one_hot_table[vocab[word]]\n",
    "\n",
    "    def build_ds(self, words):\n",
    "        targets, contexts = None, None\n",
    "        for i in range(self.half_context_size, self.text_size-self.half_context_size):\n",
    "            context_left = torch.vstack([self.word_to_tensor(words[i -self.half_context_size + j],\n",
    "                                                        self.vocab_dict) for j in range(self.half_context_size)])\n",
    "            context_right = torch.vstack([self.word_to_tensor(words[i + j + 1],\n",
    "                                                        self.vocab_dict) for j in range(self.half_context_size)])\n",
    "            context = torch.vstack((context_left, context_right)).to(torch.long)\n",
    "\n",
    "            #print(f\"DEBUG [DS]: {[words[j] for j in range(i-self.half_context_size, i+self.half_context_size)]}\")\n",
    "\n",
    "            target = self.vocab_dict[words[i]]\n",
    "            target = torch.tensor(target, dtype=torch.long).reshape(1,)\n",
    "\n",
    "            if contexts is None:\n",
    "                contexts = context.reshape(1, self.context_size)#, self.vocab_size)\n",
    "                targets = target\n",
    "            else:\n",
    "                contexts = torch.vstack((contexts, context.reshape(1, self.context_size)))#, self.vocab_size)))\n",
    "                targets = torch.vstack((targets, target))\n",
    "\n",
    "        print(f\"[DS] Last text word id: {i}\")\n",
    "        print(f\"[DS] contexts: {contexts.size()}, type: {contexts.type()}\")\n",
    "        print(f\"[DS] targets: {targets.size()}, type: {targets.type()}\")\n",
    "        print(f\"[DS] vocab_size: {self.vocab_size}\")\n",
    "        return contexts, targets\n",
    "\n",
    "\n",
    "    def __init__(self, words, context_size, dtype=torch.float32):\n",
    "        self.text_size = len(words)\n",
    "        print(f\"[DS] text size: {self.text_size}\")\n",
    "        self.dtype = dtype\n",
    "        self.context_size = int(context_size)\n",
    "        self.half_context_size = int(context_size/2)\n",
    "        vocab = list(set(words))\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.vocab_dict = {name: i for i, name in enumerate(vocab)}\n",
    "        # one-hot table to generate vocab-size targets\n",
    "        self.one_hot_table = torch.eye(self.vocab_size, self.vocab_size)\n",
    "\n",
    "        #build_ds = jax.jit(self.build_ds_jax)\n",
    "        build_ds = self.build_ds\n",
    "        self.contexts, self.targets = build_ds(words)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # inputs are (text_size-context_size)/context_size/int\n",
    "        # targets are (text_size-context_size)/1/vocab_size\n",
    "        context = self.contexts[idx]\n",
    "        target = self.targets[idx]\n",
    "        return context, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset text source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sources = [\"shakespeare\", \"The Three Little Pigs\", \"dataset\"]\n",
    "# pick one of above\n",
    "#text_source = \"The Three Little Pigs\"\n",
    "text_source = \"shakespeare\"\n",
    "\n",
    "\n",
    "match text_source:\n",
    "\n",
    "    case \"shakespeare\":\n",
    "        text_url = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
    "        text_file = \"shakespeare.txt\"\n",
    "        text_words_file = \"text_words.pkl\"\n",
    "        text_size = 120_567\n",
    "\n",
    "        if Path(text_words_file).exists():\n",
    "            with open(text_words_file, \"rb\") as file:\n",
    "                text_words = pickle.load(file)\n",
    "        else:\n",
    "            urllib.request.urlretrieve(text_url, text_file)\n",
    "            with open(file=text_file, mode='r') as file:\n",
    "                text = file.read()\n",
    "                # split\n",
    "                text_words = list(text.split())\n",
    "                # save for quick retrieval\n",
    "                with open(text_words_file, 'wb') as file:\n",
    "                    pickle.dump(text_words, file)\n",
    "\n",
    "        text_words = text_words[:text_size]\n",
    "\n",
    "        probe_word = \"lartius\"\n",
    "\n",
    "\n",
    "    case \"The Three Little Pigs\":\n",
    "        text = \"\"\"\n",
    "                            The Three Little Pigs\n",
    "                            ---------------------\n",
    "                            Once upon a time\n",
    "                            there was a family of three little pigs\n",
    "                                *oink sound effect*\n",
    "                            one little pig lived in a house of straw\n",
    "                            another little pig lived in a house of wood\n",
    "                            the third little pig lived in a house of brick\n",
    "                            But then there came along a big bad wolf.\n",
    "                                *howling wolf*\n",
    "                            The wolf went knocking on the first pig's door\n",
    "                            \"Little pig, little pig,\" he said.  \"let me come in!\"\n",
    "                            \"Not by the hairs on my chinny chin chin.\" the first pig said.\n",
    "                            \"Then I'll huff, and I'll puff, and I'll blow your house in,\" said the wolf.\n",
    "                            So he huffed, and he puffed,\n",
    "                                *wind sound effect*\n",
    "                            and he blew the house down and ate the little pig right up.\n",
    "                            The next day, the big bad wolf came knocking on the second pig's door.\n",
    "                                *knock sound effect*\n",
    "                            \"Little pig, little pig,\" he said, \"let me come in!\"\n",
    "                            \"Not by the hairs on my chinny chin chin,\" the second pig said.\n",
    "                            \"Then I'll huff and I'll puff, and I'll blow your house in,\" said the wolf.\n",
    "                            So he huffed and he puffed,\n",
    "                                *wind sound effect*\n",
    "                            and he blew the house down, and ate the little pig right up.\n",
    "                            The next day, the big bad wolf came knocking on the third pig's door.\n",
    "                            \"Little pig, little pig,\" he said, \"let me come in!\"\n",
    "                            \"Not by the hairs on my chinny chin chin,\" the third pig said.\n",
    "                            \"Then I'll huff and I'll puff, and I'll blow your house in,\" said the wolf.\n",
    "                            So he huffed and he puffed,\n",
    "                                *wind sound effect*\n",
    "                            but the house of bricks would not fall.\n",
    "                            The wolf went home and thought about how to trick the third little pig.\n",
    "                            The next day, the big bad wolf came knocking on the door again.\n",
    "                            \"Ho ho ho,\" he said. \"It's me, Santa Claus! I have presents!\"\n",
    "                            \"If you are really Santa, then come down my chimney chim chim,\" the third pig said.\n",
    "                            So he jumped up on the roof and climbed in the chimney.\n",
    "                            But the third little pig knew it was the wolf, and put a pot of hot water under the chimney.\n",
    "                            Down the wolf came, straight into the water, and got cooked right up.\n",
    "                                *screaming, burning alive sound effect*\n",
    "                            As the pig began to feast on his wolf dinner, his brothers jumped out and they were reunited.\n",
    "                            And the three little pigs lived happily ever after.\n",
    "                                *music*\n",
    "\n",
    "                        \"\"\"\n",
    "        text_words = list(text.split())\n",
    "        probe_word = \"wolf\"\n",
    "\n",
    "    case \"dataset\":\n",
    "        print(\"Using text from dataset/dataset.txt\")\n",
    "        with open(\"dataset/dataset.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "            text_words = list(text.split()[:-9421])\n",
    "            probe_word = \"empathy\"\n",
    "\n",
    "\n",
    "    case _:\n",
    "        print(\"Using short text from beginning of notebook\")\n",
    "        probe_word = \"mathematicians\"\n",
    "        text_words = list(text.split())\n",
    "\n",
    "text_words = [word.lower() for word in text_words if word.isalpha()]\n",
    "print(f\"test len: {len(text_words)}, first 10 words: {text_words[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for optimal params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Set hyperparameters\n",
    "    embed_dim = trial.suggest_int('embed_dim', 5, 55, step=10)\n",
    "    learning_rate = trial.suggest_float('lr', 1e-3, 1, log=True)\n",
    "    #epochs = trial.suggest_int('epochs', 1, 2)\n",
    "    epochs = 10\n",
    "    half_context_size = trial.suggest_int('half_context_size', 2, 5)\n",
    "    context_size = 2*half_context_size\n",
    "    batch_size=len(text_words) - context_size\n",
    "    seed = trial.suggest_int('seed', 70, 75)\n",
    "\n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    # Build training list for Word2Vec Continuous Bag-of-Words method\n",
    "    print(f\"[objective] batch_size: {batch_size}\")\n",
    "    print(f\"[objective] embed_dim: {embed_dim}\")\n",
    "    print(f\"[objective] context size: {context_size}\")\n",
    "\n",
    "    if context_size in datasets:\n",
    "        dataset = datasets[context_size]\n",
    "    else:\n",
    "        dataset = CBOWTrainingDataset(text_words, context_size)\n",
    "        datasets[context_size] = dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Verify sizes\n",
    "    for context, target in dataloader:\n",
    "        print(f\"[objective] context: {context.size()}, \\ntarget: {target.size()}, \\nvocab_size: {dataset.vocab_size}\")\n",
    "        break\n",
    "\n",
    "\n",
    "    # Define model, loss function and optimizer\n",
    "    model = CBOW(dataset.vocab_size, embedding_dim=embed_dim, context_size=context_size, batch_size=batch_size, seed=seed)\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training\n",
    "    _, losses = train(dataloader=dataloader,\n",
    "                   model=model,\n",
    "                   loss_fn=loss_fn,\n",
    "                   optimizer=optimizer,\n",
    "                   epochs=epochs,\n",
    "                   batch_size=batch_size,\n",
    "                   device=device,\n",
    "                   print_every=10\n",
    "                   )\n",
    "\n",
    "    return losses[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run grid search and/or training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [\"grid_seach_mode\", \"train\", \"load\", \"load_and_train\"]\n",
    "mode = \"load\"\n",
    "# Grid search is very long, it looks for optimal hyperparameters\n",
    "# Once you found optimal parameters disable grid_search_mode and set below to optimal (they will be ignored if you choose \"grid_search_mode\")\n",
    "params = {'embed_dim': 100, 'lr': 0.00009013992123647334, 'half_context_size': 5, 'seed': 72}\n",
    "\n",
    "# datasets dict with context_size as key and CBOWTrainingDataset instance for that context size as value - they are reused in the optuna trials or during final training\n",
    "datasets = {}\n",
    "\n",
    "match mode:\n",
    "    case \"grid_seach_mode\":\n",
    "        # Optuna study for hyperparameter optimization\n",
    "        study = optuna.create_study(storage=\"sqlite:///db.sqlite3\",\n",
    "                                    study_name=\"Embeddings-similarity2\",\n",
    "                                    direction=\"minimize\",\n",
    "                                    load_if_exists = True)\n",
    "        study.optimize(objective, n_trials=100)\n",
    "\n",
    "        print(f\"Best value: {study.best_value} (params: {study.best_params})\")\n",
    "        embed_dim = study.best_params['embed_dim']\n",
    "        lr = study.best_params['lr']\n",
    "        context_size = 2*study.best_params['half_context_size']\n",
    "        #batch_size = study.best_params['batch_size']\n",
    "        seed = study.best_params['seed']\n",
    "\n",
    "        params = study.best_params\n",
    "\n",
    "    case \"train\":\n",
    "\n",
    "        embed_dim = params['embed_dim']\n",
    "        lr = params['lr']\n",
    "        context_size = 2*params['half_context_size']\n",
    "        batch_size=len(text_words) - context_size\n",
    "        batch_size = 10000\n",
    "        seed = params['seed']\n",
    "\n",
    "        epochs = 1000\n",
    "\n",
    "\n",
    "        # Build training list for Word2Vec Continuous Bag-of-Words method\n",
    "        print(f\"[objective] batch_size: {batch_size}\")\n",
    "        print(f\"[objective] embed_dim: {embed_dim}\")\n",
    "        print(f\"[objective] context size: {context_size}\")\n",
    "\n",
    "        if context_size in datasets:\n",
    "            dataset = datasets[context_size]\n",
    "        else:\n",
    "            dataset = CBOWTrainingDataset(text_words, context_size)\n",
    "            datasets[context_size] = dataset\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Verify sizes\n",
    "        for context, target in dataloader:\n",
    "            print(f\"[objective] context: {context.size()}, \\ntarget: {target.size()}, \\nvocab_size: {dataset.vocab_size}\")\n",
    "            break\n",
    "\n",
    "\n",
    "        # Define model, loss function and optimizer\n",
    "        model = CBOW(dataset.vocab_size, embedding_dim=embed_dim, context_size=context_size, batch_size=batch_size, seed=seed)\n",
    "        model.to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Training\n",
    "        model, losses = train(dataloader=dataloader,\n",
    "                        model=model,\n",
    "                        loss_fn=loss_fn,\n",
    "                        optimizer=optimizer,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        device=device,\n",
    "                        print_every=epochs//100,\n",
    "                        debug=False\n",
    "                    )\n",
    "\n",
    "        # Display performance\n",
    "        plt.plot(losses)\n",
    "        print(f\"Loss after last epoch: {losses[-1]}\")\n",
    "    case \"load\":\n",
    "        context_size = 2*params['half_context_size']\n",
    "        #batch_size=len(text_words) - context_size\n",
    "        batch_size = 10000\n",
    "        seed = params['seed']\n",
    "        if context_size in datasets:\n",
    "            dataset = datasets[context_size]\n",
    "        else:\n",
    "            dataset = CBOWTrainingDataset(text_words, context_size)\n",
    "            datasets[context_size] = dataset\n",
    "        model = CBOW(dataset.vocab_size, embedding_dim=embed_dim, context_size=context_size, batch_size=batch_size, seed=seed)\n",
    "        model.load_state_dict(torch.load(\"model-shakespare-back.pth\"))\n",
    "        model.to(device='mps')\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    case _:\n",
    "        raise(\"Unknown mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare vocab dictionary for embeddings visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "presentation_vocab_dict = dataset.vocab_dict\n",
    "presentation_reverse_vocab_dict = {i: name for name, i in presentation_vocab_dict.items()}\n",
    "\n",
    "dict_words = []\n",
    "#presentation_vocab = list(set(text_words[:10000]))\n",
    "presentation_vocab = list(set(text_words))\n",
    "for id, word in enumerate(presentation_vocab):\n",
    "    word_id = torch.tensor(presentation_vocab_dict[word]).to(device)\n",
    "    embed = model.get_embed(word_id)\n",
    "    # append\n",
    "    dict_row = {\"id\": id, \"word\": word, \"raw_embed\": embed}\n",
    "    dict_row.update({f\"dim_{dim_id}\": dim_val.item() for dim_id, dim_val in enumerate(embed)})\n",
    "    dict_words.append(dict_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Eye\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# eval for random sentence\n",
    "dataset = CBOWTrainingDataset(text_words, context_size)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "input, target = next(iter(dataloader))\n",
    "input_words = [dataset.vocab[id] for id in list(input.detach().squeeze().numpy())]\n",
    "print(\"Input:\", input_words)\n",
    "input = input.to(device)\n",
    "pred = model(input)\n",
    "\n",
    "pred_best = torch.argmax(pred.to(\"cpu\").detach(), dim=1)\n",
    "target = target.squeeze()\n",
    "print(\"prediction: \", dataset.vocab[pred_best], \"\\ntarget:     \", dataset.vocab[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define word-to-embedding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame.from_dict(dict_words)\n",
    "\n",
    "def word_to_embed(word):\n",
    "    return dict_words[presentation_vocab_dict[word]][\"raw_embed\"]\n",
    "\n",
    "def embed_to_word(embed):\n",
    "    return [record for record in dict_words if torch.equal(record.get(\"raw_embed\"), embed)][0][\"word\"]\n",
    "\n",
    "# Test functions - transpose first embedding to word and back\n",
    "assert(text_words[0] == embed_to_word(word_to_embed(text_words[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting similarities for particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similarities between given word and others\n",
    "#probe_word = THIS IS DEFINED ALONG WITH DATASET\n",
    "n = 25\n",
    "\n",
    "word_embed = word_to_embed(probe_word)\n",
    "\n",
    "# Build list of similarities between probe_word and rest of words from vocabulary\n",
    "similarities = []\n",
    "for word in dict_words:\n",
    "    similarities.append((word[\"word\"], F.cosine_similarity(word_embed, word[\"raw_embed\"], dim=0)))\n",
    "\n",
    "print(f\"First {n} similarities for \\\"{probe_word}\\\":\\n{similarities[:n]}\\n\")\n",
    "\n",
    "# Sort the list of tuples in descending order based on the values\n",
    "inverse = [(value, key) for key, value in similarities]\n",
    "inverse.sort(reverse=True)\n",
    "\n",
    "# Get the keys corresponding to the n greatest values\n",
    "top_n_keys = [key for _, key in inverse[:n]]\n",
    "\n",
    "print(f\"Keys corresponding to the {n} greatest values:\\n{top_n_keys}\\nand least: \\\"{inverse[-1][1]}\\\"\\n\")\n",
    "\n",
    "# Transpose to DataFrame for plotting\n",
    "df_words = pd.DataFrame(dict_words)\n",
    "print(\"DataFrame:\\n\", df_words, \"\\n\")\n",
    "\n",
    "# Plot similarities for probe_word\n",
    "plt.figure(figsize=(15, 9))  # Adjust the width and height as needed\n",
    "df_print = df_words.filter(items=['id', 'word', 'dim_0', 'dim_1', 'dim_2', 'dim_3', 'dim_4']) # plot only first 5 dimensions\n",
    "parallel_coordinates(df_print[df_print[\"word\"].isin(top_n_keys)].drop([\"id\"], axis=1), 'word')\n",
    "#plt.legend(ncol=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting through reduction of dimmensionality using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = df_words.drop(['id', 'word', 'raw_embed'], axis=1)\n",
    "labels = df_words[\"word\"]\n",
    "\n",
    "\n",
    "standardized_data = StandardScaler().fit_transform(data)\n",
    "print(standardized_data.shape)\n",
    "\n",
    "\n",
    "# Picking the top 1000 points as TSNE\n",
    "# takes a lot of time for 15K points\n",
    "#data_1000 = standardized_data[0:1000, :]\n",
    "#labels_1000 = labels[0:1000]\n",
    "\n",
    "model_tsne = TSNE(n_components = 2, learning_rate=1000, random_state = 0, perplexity=300, n_iter=1_000_000_000, early_exaggeration=30.0)\n",
    "tsne_data = model_tsne.fit_transform(standardized_data)\n",
    "\n",
    "# creating a new data frame which\n",
    "# help us in plotting the result data\n",
    "tsne_df = pd.DataFrame(data = np.vstack((tsne_data.T, labels)).T,\n",
    "     columns =(\"Dim_1\", \"Dim_2\", \"label\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(50, 50))\n",
    "ax = fig.add_subplot(1, 1, 1)#, projection='3d')\n",
    "\n",
    "ax.scatter(tsne_df['Dim_1'], tsne_df['Dim_2'])\n",
    "#parallel_coordinates(df_print[df_print[\"word\"].isin(top_n_keys)].drop([\"id\"], axis=1), 'word')\n",
    "\n",
    "# annotator function that draws a label and an arrow\n",
    "# that points from the label to its corresponding point\n",
    "def annotate(ax, label, x, y, xytext):\n",
    "    ax.annotate(label, xy=(x,y),\n",
    "                xytext=xytext, textcoords='offset points',\n",
    "                fontsize=6,\n",
    "                )\n",
    "\n",
    "# conditionally position labels\n",
    "for label, x, y in zip(tsne_df[\"label\"], tsne_df[\"Dim_1\"], tsne_df[\"Dim_2\"]):\n",
    "    if y > 0.9:\n",
    "        annotate(ax, label, x, y, (-5, -15))\n",
    "    else:\n",
    "        annotate(ax, label, x, y, (-5, 10))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4476212,
     "sourceId": 7673794,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 167621600,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
